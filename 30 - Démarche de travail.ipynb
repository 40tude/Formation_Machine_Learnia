{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Démarche de travail\n",
    "\n",
    "* Projet Covid\n",
    "* Telecharger depuis Kaggle\n",
    "* https://www.kaggle.com/datasets/einsteindata4u/covid19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Démarche\n",
    "1. Définir un objectif mesurable\n",
    "    * Objectif : mesurer si une personne est infectée en fonction des données cliniques dispo\n",
    "    * Métrique : précision 90%\n",
    "\n",
    "* Attention. Dans ce type de dataset on a très peu de contaminés et beaucoup de non contaminé.\n",
    "* C'est facile d'obtenir 90%. Y a qu'à dire que le modèle c'est que personne n'est contaminé\n",
    "\n",
    "\n",
    "1. Définir un objectif mesurable\n",
    "    * Objectif : mesurer si une personne est infectée en fonction des données cliniques dispo\n",
    "    * Métrique : ~~précision 90%~~ \n",
    "        * Précision : réduire au max le taux de faux positifs. On limite le nb de personnes saines envoyées à l'hopital\n",
    "        * Recall (sensibilité) : réduire au max le taux de faux négatifs\n",
    "        * Score F1 (ratio précision/sensibilité). On limite le nombre de personnes contaminées qui restent dans la nature  \n",
    "    * On décide F1=50% et Recall=70%\n",
    "\n",
    "Mesure de performance calculé à partir des éléments dispos dans une matrice de confusion.\n",
    "\n",
    "![Alt text](assets/demarche1.png)\n",
    "\n",
    "\n",
    "2. EDA - Exploratory Data Analysis\n",
    "    * Analyse et exploration des données\n",
    "    * Comprendre les variables et le dataset\n",
    "    * Pour définir une stratégie de modélisation : qu'est-ce qu'on va faire avec les données pour atteindre l'objectif\n",
    "1. Pre-processing\n",
    "    * Voir 24 - scikit-learn_Preprocessing_Pipeline.ipynb\n",
    "    * Voir vidéo 22/30\n",
    "    * Objectif : transformer le dataset pour le mettre dans un format prpopice au developpement de modèles de ML\n",
    "    * Encodage, eliminer variable manquante, selection de variables\n",
    "1. Modeling\n",
    "    * Créer un modèle\n",
    "    * L'entrainer\n",
    "    * L'évaluer\n",
    "    * L'améliorer si possible \n",
    "        * sélectionnant d'autres variables\n",
    "        * revoir le preprocessing\n",
    "        * comparer ce modèle avec d'autres\n",
    "        * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check List à suivre\n",
    "\n",
    "* Prendre des notes \n",
    "    * au fil de l'eau et les regrouper\n",
    "    * rédiger une conclusion\n",
    "    \n",
    "## Objectif \n",
    "* comprendre les données dont on dispose \n",
    "* pour définir une stratégie de modélisation\n",
    "\n",
    "Commencer simple !\n",
    "\n",
    "## Analyse de la forme\n",
    "* Identification de la target\n",
    "* Nombre de lignes et de colonnes\n",
    "* Types de variables\n",
    "    * Nombre de variables discrètes, continues\n",
    "    * df.dtypes et df.dtypes.value_counts()\n",
    "* Identification des valeurs manquantes\n",
    "    * Où sont elles dans le dataset?\n",
    "\n",
    "## Analyse du fond\n",
    "* Elimination des colonnes inutiles \n",
    "* Visualisation de la target\n",
    "* Compréhension des différentes variables (web)\n",
    "* Visualisation des relations feature-target \n",
    "    * histo, boxplot selon que la target est continue/discrete et que la variable est continue/discrete\n",
    "    * Si on a 2 discrete, penser à pd.crosstab() & sns.heatmap(pd.crosstab...)\n",
    "* Identification des outliers\n",
    "\n",
    "À la fin faut être à l'aise avec les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template des notes\n",
    "\n",
    "* Prendre des notes \n",
    "    * au fil de l'eau et les regrouper\n",
    "    * rédiger une conclusion\n",
    "    \n",
    "### Objectif \n",
    "* comprendre les données dont on dispose \n",
    "* pour définir une stratégie de modélisation\n",
    "\n",
    "Commencer simple !\n",
    "\n",
    "### Analyse de la forme\n",
    "* Identification de la target\n",
    "* Nombre de lignes et de colonnes\n",
    "    * shape, ndim\n",
    "* Types de variables\n",
    "    * Nombre de variables discrètes, continues\n",
    "    * df.dtypes et df.dtypes.value_counts()\n",
    "* Identification des valeurs manquantes\n",
    "    * Où sont elles dans le dataset? sns.heatmap(df.isna())\n",
    "    * Les compter : df.dtypes.value_counts(), (df.isna().sum()/df.shape[0]).sort_values(ascending=True)\n",
    "\n",
    "### Analyse du fond\n",
    "* Elimination des colonnes inutiles \n",
    "    * Bool indexing df = df[df.columns[df.isna().sum()/df.shape[0] < .9]]\n",
    "    * Les lignes à valeur constantes etc.\n",
    "* Visualisation de la target\n",
    "    * df[\"SARS-Cov-2 exam result\"].value_counts(normalize=True) \n",
    "* Compréhension des différentes variables\n",
    "    * Infos du domaine depuis le Web\n",
    "    * Histogrammes des variables continues\n",
    "        * sns.histplot(df[col], label=\"positive\", kde=True, stat=\"density\", kde_kws=dict(cut=3), alpha=.4, edgecolor=(1, 1, 1, .4)) \n",
    "    * Variables catégorielles\n",
    "        * Lister les catégories de chaque variable\n",
    "          * for col in df.select_dtypes(\"object\"):\n",
    "              print(f\"{col :.<50} : {df[col].unique()}\")\n",
    "        * Pour chaque variable, compter le nombre de valeurs\n",
    "          * for col in df.select_dtypes(\"object\"):\n",
    "              print(df[col].value_counts(normalize=True))\n",
    "          * Sortir un camembert ?\n",
    "* Visualisation des relations feature-target \n",
    "    * Penser à faire des sous-catégories \n",
    "        * boolean indexing. Ex : blood_columns = df.columns[(missing_rate < .9) & (missing_rate>0.88)]\n",
    "        * A afficher la relation feature-target pour chaque élément de la sous-catégorie\n",
    "    * histo, boxplot selon que la target est continue/discrete et que la variable est continue/discrete\n",
    "    * penser à sns.countplot() avec hue=\"MyTarget\"\n",
    "    * Si on a 2 discretes, penser à pd.crosstab() & sns.heatmap(pd.crosstab...)\n",
    "* Identification des outliers\n",
    "\n",
    "### Conclusion\n",
    "* À la fin faut être à l'aise avec les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "**Objectif :** transformer le dataset pour le mettre dans un format propice au developpement de modèles de ML\n",
    "\n",
    "* Création des Train set et Test set\n",
    "* Eliminer les Nan : dropna(), imputation, supprimer les colonnes vides ou constantes\n",
    "* Optionnel à ce stade : Encoder les données catégorielles (OneHotEncoding()...)\n",
    "* Optionnel à la première itération - Supprimer les outliers néfastes au modèle\n",
    "    * Peut être fait une fois qu'on a déjà un premier modèle qu'on souhaite améliorer\n",
    "    * Alors à ce moment il peut être judicieux de virer les outliers\n",
    "* Features Selection \n",
    "    * selection de variables\n",
    "    * Supprimer les variables dont la variance est faible\n",
    "    * Supprimer les variables redondantes\n",
    "* Features engineering \n",
    "    * création de variables à partir des variables existantes \n",
    "    * faut être un \"domain expert\"  \n",
    "    * utiliser polynomial feature avec le transfomrer de sklearn\n",
    "    * Utiliser un PCA pour réduire le nb de dimensions du dataset\n",
    "* Features scaling\n",
    "    * Normaliser les données avec un MinMaxScaler() ou un StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "**Objectif :** déveloper un modèle de ML qui répond à l'objectif final\n",
    "\n",
    "* Définir une fonction d'évaluation\n",
    "    * Doit être fiable et reproductuble\n",
    "    * Doit permettre de comparer différents modèles\n",
    "    * Qui peuvent fonctionner sur différentes config du dataset    * \n",
    "* Entrainement de différents modèles\n",
    "    * Créer une liste de modèle de ML et de les entrainer puis de les comparer avec le système d'évaluation\n",
    "    * On retient les modèles aux meilleures perfs\n",
    "* Optimisation avec GridSearchCV()\n",
    "    * optimiser les hyper paramètres des modèles retenus\n",
    "* Optionnel - Analyse des erreurs et retour au preprocessing & EDA\n",
    "    * Règler le dataset pour augmenter les perfs\n",
    "* Learning curve et prise de décision\n",
    "    * Toujours vérifier la learning curve du modèle\n",
    "    * Faut il recolter plus de données pour augmenter la perf (ou non)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
